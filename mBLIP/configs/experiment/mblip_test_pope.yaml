# @package _global_
defaults:
  - /evaluation/mblip_eval_classification@module.evaluation
  - override /trainer: default.yaml # choose trainer from 'configs/trainer/'
#  - override /module: clip_distill
  - override /datamodule: mblip_halu_pope
  - override /callbacks: null
  - override /config_callbacks: trident.yaml
  - override /logger: wandb #csv #wandb.yaml

seed: 42
train: False

data_prefix: /media/gregor/DATA/projects/wuerzburg #/media/gregor/DATA/projects/wuerzburg # "D:" "D:/Research/projects/wuerzburg"
train_image_root: ${data_prefix}/mblip/data/pretrain/images_1m #${data_prefix}/mblip/data/images_1m  # ${data_prefix}/zeroshot-image/babel/data/distillation/images_ccs_100000
train_data: ${data_prefix}/mblip/data
xm3600_image_root: ${data_prefix}/iglue/datasets/Crossmodal3600/images
flickr_image_root: ${data_prefix}/iglue/datasets/flickr30k/flickr_images
gqa_image_root: ${data_prefix}/iglue/datasets/gqa/images

blip_checkpoint: ${data_prefix}/mblip/checkpoints/blip2-flant5xl-nolm.bin
blip_model: Salesforce/blip2-flan-t5-xl
llm:  bigscience/mt0-xl #google/flan-t5-xl #bigscience/mt0-xl #bigscience/bloomz-7b1-mt
tokenizer_name: ${llm}

trainer:
  num_sanity_val_steps: 0
  max_epochs: 5
  devices: 1
  precision: 16
  gradient_clip_val: 1.0
  accumulate_grad_batches: 4
  log_every_n_steps: 10
  val_check_interval: 0.5
#  strategy: deepspeed_stage_2
#  limit_train_batches: 5
#  limit_val_batches: 10
#  limit_test_batches: 10


datamodule:
  dataloader_cfg:
    train:
      batch_size: 64
      num_workers: 2
    val:
      batch_size: 8
      num_workers: 2
    test:
      batch_size: 16
      num_workers: 2
    num_workers: 2
module:
  model:
    _target_: src.modules.modeling.mblip.mBLIPModule #src.modules.modeling.clip_model.TextDistillation  #src.tasks.distill.distill.Head
    blip_pretrained: ${blip_model}
    blip_pretrained_checkpoint: ${blip_checkpoint}
    lm_pretrained: /media/gregor/DATA/projects/wuerzburg/mblip/checkpoints/mt0-xl/05_29_2023_16_52_44-1-60663 #${llm}
    #/media/gregor/DATA/projects/wuerzburg/mblip/checkpoints/mt0-xl/05_18_2023_16_31_57-0-20595 #${llm}
    train_checkpoint: /media/gregor/DATA/projects/wuerzburg/mblip/checkpoints/05_29_2023_16_52_44/checkpoints/1-60663.ckpt
    #05_18_2023_16_31_57/checkpoints/0-20595.ckpt
    #/media/gregor/DATA/projects/wuerzburg/trident-iglue/logs/runs/2023-05-16/16-56-30/checkpoints/0-8016.ckpt
#    lora_checkpoint: /media/gregor/DATA/projects/wuerzburg/mblip/checkpoints/xvnli-05_24_2023_10_22_00/5-23242
  #/media/gregor/DATA/projects/wuerzburg/mblip/checkpoints/05_18_2023_16_31_57/checkpoints/0-14416/adapter_model.bin
    load_8bit: True
    freeze_vit: True
    freeze_lm: True
    freeze_qformer: False
    freeze_projection: False
    gradient_checkpoint: True
    use_lora: False
    lora_bias: none
    lora_alpha: 16
    lora_r: 8
    lora_dropout: 0.05
  optimizer:
    lr: 0.00001  #0.0005
    weight_decay: 0.1
  scheduler:
    _target_: transformers.optimization.get_cosine_schedule_with_warmup
    num_warmup_steps: 1000

logger:
  csv:
    save_dir: ${hydra:runtime.output_dir}
  wandb:
    name: "test=pope_${hydra:runtime.output_dir}"
    project: "mblip"


callbacks:
  model_checkpoint_on_epoch:
    _target_: src.tasks.vllm.checkpoint.mBLIPModelCheckpoint #lightning.pytorch.callbacks.ModelCheckpoint # src.tasks.distill.distill.AdapterModelCheckpoint
    freeze_vit: ${module.model.freeze_vit}
    freeze_qformer: ${module.model.freeze_qformer}
    freeze_lm: ${module.model.freeze_lm}
    freeze_projection: ${module.model.freeze_projection}
    lora: True
#    monitor: "" # name of the logged metric which determines when model is improving
    every_n_epochs: 1
    verbose: false
    save_top_k: -1 # -1 -> all models are saved
    save_last: false # additionaly always save model from last epoch
    dirpath: "${hydra:runtime.output_dir}/checkpoints/"
    auto_insert_metric_name: false
  teardown:
    _target_: src.tasks.vllm.checkpoint.TeardownCallback
